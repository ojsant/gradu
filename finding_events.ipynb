{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e395c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "import traceback\n",
    "from time import sleep\n",
    "import requests\n",
    "\n",
    "from anisotropy import run_SEPevent\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "filehandler = logging.FileHandler(filename=\"wind_events_test.log\", encoding=\"utf-8\")\n",
    "streamhandler = logging.StreamHandler(sys.stdout)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s', \n",
    "                    datefmt='%m/%d/%Y %H:%M:%S', handlers=[filehandler, streamhandler], force=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"WARNING: background mean is nan!\", category=UserWarning, module=\"pyonset\")\n",
    "\n",
    "# def check_for_gaps(data, start, end, max_gap_mins=10):\n",
    "#     flux_finite = data.dropna()\n",
    "#     for i in range(len(flux_finite[start:end]) - 1):\n",
    "#         if flux_finite.index[i + 1] - flux_finite.index[i] > pd.Timedelta(minutes=max_gap_mins):\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "year = 2012\n",
    "\n",
    "data_path = f\"{os.getcwd()}{os.sep}data\"\n",
    "csv_path = f\"{os.getcwd()}{os.sep}wind_events_{year}.csv\"\n",
    "plot_dir = f\"{os.getcwd()}{os.sep}/plots/{year}\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "d = 0   # day counter\n",
    "j = 0   # retry counter\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "while d < 365:\n",
    "    try:\n",
    "        onset_found = False\n",
    "        resample = \"5min\"\n",
    "        resample_int = int(resample.removesuffix(\"min\"))\n",
    "        window_len = 1\n",
    "        onsets = []\n",
    "        peaks = []\n",
    "\n",
    "        date = pd.to_datetime(f\"{year}-01-01\") + pd.Timedelta(days=d)\n",
    "        next_date = date + pd.Timedelta(days=1)\n",
    "        logging.info(f\"Analyzing flux for {date}\")\n",
    "\n",
    "        start = date - pd.Timedelta(days=1)\n",
    "        end = date + pd.Timedelta(days=2)\n",
    "\n",
    "        # d = start.strftime(\"%Y%m%d\")\n",
    "        # while d != end:\n",
    "        #     rc = subprocess.call(f'wget -r -nv --show-progress --tries=10 -nc -nH -np -nd -P \"data\" -A \"*_{d}_*.cdf\" \"https://cdaweb.gsfc.nasa.gov/pub/data/wind/3dp/3dp_sfpd/{year}/\"'.split(\" \"))\n",
    "        \n",
    "        event = run_SEPevent(data_path, spacecraft_instrument=\"Wind 3DP\", starttime=start, endtime=end, \n",
    "                            species=\"e\", channels=3, averaging=resample)         # wget data from CDAWeb if SSL Berkeley is down\n",
    "        \n",
    "        # Messy workaround.\n",
    "        data = pd.Series(event.I_data[:,3], index=event.I_times).ffill(limit=2)\n",
    "        event.wind_peak_removal(n_lim=2)   # Wind has some high flux peaks lasting for a couple of minutes. Remove these\n",
    "        data = pd.Series(event.I_data[:,3], index=event.I_times).ffill(limit=2) # do this twice since first removal doesnt work if preceding value is missing -> fill missing values with previous values\n",
    "        \n",
    "        # 4-sigma method (Krucker et al. 1999, modified)        # TODO: 2.12. 3-sigma and 15 min averaging instead\n",
    "        i = 0\n",
    "        while i < len(data[date:next_date] - 1):\n",
    "            # 1: calculate window mean and std\n",
    "            window_end = date + pd.Timedelta(minutes=resample_int*i) # first window ends at the end of the 1st day\n",
    "            window_start = window_end - pd.Timedelta(hours=window_len)\n",
    "            # if check_for_gaps(data, window_start, window_end, max_gap_mins=10): \n",
    "            #     if not data_gaps_present:\n",
    "            #         logging.info(\"Data gaps present, onset not determined\")\n",
    "            #         data_gaps_present = True\n",
    "            #     continue\n",
    "            # else:\n",
    "            #     data_gaps_present = False\n",
    "            window = data[window_start:window_end]\n",
    "            window_mean = window.mean()\n",
    "            window_std = window.std()            \n",
    "            \n",
    "            # 2: z-standardise\n",
    "            \n",
    "            I_z = (data[window_end:].dropna() - window_mean) / window_std\n",
    "            \n",
    "            peak_flux = np.nanmax(data[window_start:window_end + pd.Timedelta(hours=2)])  # peak flux within window_len + 2 hours\n",
    "            peaks.append(peak_flux)\n",
    "            # 3: check next point for 4 sigma deviation. If true, take the last timestamp of bg subtracted flux < 0 as onset\n",
    "            # or maybe check if few of the next points are all above 4 sigma, so flukes won't trigger the method\n",
    "            next_points = I_z.iloc[0:5]     # next 5 points\n",
    "                \n",
    "            if (next_points > 4).sum() == len(next_points): \n",
    "                onset_found = True\n",
    "                onset_result = window.index[window <= window_mean][-1]\n",
    "                logging.info(f\"Found onset with background {window_start} - {window_end}: {onset_result}\")\n",
    "                onsets.append(onset_result)\n",
    "                # Write to CSV\n",
    "                with open(csv_path, \"+r\", encoding=\"utf-8\") as fp:\n",
    "                    event_no = len(fp.readlines())\n",
    "                    fp.write(f\"{event_no},{onset_result.date()},{onset_result.time()},{peak_flux}\\n\")\n",
    "                    logging.info(f\"Results successfully saved to {csv_path}\")\n",
    "                    fp.close()\n",
    "\n",
    "                # Once an onset has been found, skip 2 hours ahead.\n",
    "                i += 120 // resample_int\n",
    "            \n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax.step(data[date:next_date].index, data[date:next_date])\n",
    "        if onset_found:\n",
    "            for time in onsets: \n",
    "                ax.axvline(time, color=\"red\")\n",
    "\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_ylim(None, 1e2 if max(peaks) < 1e2 else None)\n",
    "        ax.set_ylabel(\"Intensity\")\n",
    "        fig.suptitle(f\"Onset determination for {date}\")\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        fname = plot_dir + os.sep + f\"Wind_{date.strftime(\"%Y%m%d\")}\"\n",
    "        fig.savefig(fname, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        d += 1\n",
    "\n",
    "    except requests.exceptions.ReadTimeout:\n",
    "        logging.info(f\"{traceback.format_exc()}\")\n",
    "        if j < MAX_RETRIES:\n",
    "            j += 1\n",
    "            sleep(60)\n",
    "        else:\n",
    "            logging.info(\"Download retries exceeded, quitting...\")\n",
    "            break\n",
    "    except Exception:\n",
    "        logging.info(f\"{traceback.format_exc()}\")\n",
    "        d += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
